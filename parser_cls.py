import asyncio
import html
import json
import random
import time
import urllib3
from urllib.parse import unquote, urlparse, parse_qs, urlencode, urlunparse
from bs4 import BeautifulSoup
from curl_cffi import requests
from curl_cffi.requests import RequestsError
from loguru import logger
from pydantic import ValidationError
from requests.cookies import RequestsCookieJar
from common_data import HEADERS
from dto import Proxy, AvitoConfig
from get_cookies import get_cookies
from load_config import load_avito_config
from models import ItemsResponse, Item
from avito_db import AvitoDB
from price_manager import check_and_update_prices, get_bid_info
from init_ads import init_db_from_config
# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

DEBUG_MODE = False

logger.add("logs/app.log", rotation="5 MB", retention="10 days", level="DEBUG")


class AvitoParse:
    def __init__(
            self,
            config: AvitoConfig,
            stop_event=None
    ):
        # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.config = config
        self.stop_event = stop_event

        # –ü—Ä–æ–∫—Å–∏
        self.mobile_proxies = self._load_mobile_proxies()
        self.current_proxy_index = 0
        self.proxy_obj = self.get_current_proxy_obj()

        # HTTP / —Å–µ—Å—Å–∏—è / cookies
        self.cookies = None
        self.session = requests.Session()

        # –°—á–µ—Ç—á–∏–∫–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.requests_count = 0
        self.max_requests_per_ip = getattr(config, 'proxy_max_requests_per_rotation', 20)
        self.failed_requests_count = 0  # –°—á–µ—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥
        self.proxy_requests_count = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–∫—Å–∏
        self.last_ip_change = 0  # –º–µ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–º–µ–Ω—ã IP

        # –†–∞–±–æ—Ç–∞ —Å –ë–î
        self.db = AvitoDB()

    def _load_mobile_proxies(self) -> list:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        if hasattr(self.config, 'mobile_proxies') and self.config.mobile_proxies:
            active_proxies = [proxy for proxy in self.config.mobile_proxies if proxy.active]
            if active_proxies:
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(active_proxies)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏")
                return active_proxies

        # Fallback –∫ —Å—Ç–∞—Ä–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if all([self.config.proxy_string, self.config.proxy_change_url]):
            from dto import MobileProxy
            fallback_proxy = MobileProxy(
                proxy_string=self.config.proxy_string,
                proxy_change_url=self.config.proxy_change_url,
                name="Legacy Proxy",
                active=True
            )
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è legacy –ø—Ä–æ–∫—Å–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
            return [fallback_proxy]

        logger.warning("–ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        return []

    def get_current_proxy_obj(self) -> Proxy | None:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–∫—Å–∏"""
        if not self.mobile_proxies:
            logger.info("–†–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
            return None
        current_proxy = self.mobile_proxies[self.current_proxy_index]
        return Proxy(
            proxy_string=current_proxy.proxy_string,
            change_ip_link=current_proxy.proxy_change_url
        )

    def rotate_proxy(self) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏ –≤ —Å–ø–∏—Å–∫–µ"""
        if len(self.mobile_proxies) <= 1:
            return False
        old_index = self.current_proxy_index
        rotation_mode = getattr(self.config, 'proxy_rotation_mode', 'round_robin')
        if rotation_mode == 'random':
            available_indices = [i for i in range(len(self.mobile_proxies)) if i != self.current_proxy_index]
            if available_indices:
                self.current_proxy_index = random.choice(available_indices)
        else:
            self.current_proxy_index = (self.current_proxy_index + 1) % len(self.mobile_proxies)
        self.proxy_obj = self.get_current_proxy_obj()
        self.proxy_requests_count = 0
        current_proxy = self.mobile_proxies[self.current_proxy_index]
        logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å —Å –ø—Ä–æ–∫—Å–∏ #{old_index} –Ω–∞ –ø—Ä–æ–∫—Å–∏ #{self.current_proxy_index} ({current_proxy.name})")
        return True

    def get_proxy_obj(self) -> Proxy | None:
        return self.get_current_proxy_obj()

    def get_cookies(self, max_retries: int = 5, delay: float = 2.0) -> dict | None:
        for attempt in range(1, max_retries + 1):
            try:
                cookies = asyncio.run(get_cookies(proxy=self.proxy_obj, headless=True))
                if cookies and isinstance(cookies, dict) and len(cookies) > 0:
                    logger.info(f"[get_cookies] –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã cookies —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt}")
                    return cookies
                else:
                    raise ValueError("–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç cookies –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")
            except Exception as e:
                logger.warning(f"[get_cookies] –ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries:
                    logger.info(f"[get_cookies] –û–∂–∏–¥–∞–Ω–∏–µ {delay * attempt} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(delay * attempt)
                else:
                    logger.error(f"[get_cookies] –í—Å–µ {max_retries} –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å")
                    return None

    def save_cookies(self) -> None:
        with open("cookies.json", "w") as f:
            json.dump(self.session.cookies.get_dict(), f)

    def load_cookies(self) -> None:
        try:
            with open("cookies.json", "r") as f:
                cookies = json.load(f)
                jar = RequestsCookieJar()
                for k, v in cookies.items():
                    jar.set(k, v)
                self.session.cookies.update(jar)
        except FileNotFoundError:
            pass

    def fetch_data(self, url: str, retries: int = 5, backoff_factor: float = 1) -> str | None:
        proxy_data = None
        if self.proxy_obj:
            proxy_data = {"https": f"http://{self.proxy_obj.proxy_string}"}
        base_jitter = random.uniform(0.15, 0.4)
        time.sleep(base_jitter)
        for attempt in range(1, retries + 1):
            if self.stop_event and self.stop_event.is_set():
                return
            try:
                response = self.session.get(
                    url=url,
                    headers=HEADERS,
                    proxies=proxy_data,
                    cookies=self.cookies,
                    timeout=30,
                    verify=False,
                    allow_redirects=True
                )
                logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt}: {response.status_code}")
                if response.status_code == 200:
                    self.failed_requests_count = 0
                    self.save_cookies()
                    time.sleep(random.uniform(0.25, 0.8))
                    return response.text
                if response.status_code >= 500:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ {response.status_code}")
                    raise RequestsError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {response.status_code}")
                if response.status_code == 429:
                    logger.warning(f"Rate limit {response.status_code}")
                    self.failed_requests_count += 1
                    if time.time() - self.last_ip_change > 15:
                        if self.change_ip(max_attempts=2):
                            self.last_ip_change = time.time()
                            continue
                    raise RequestsError(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {response.status_code}")
                if response.status_code in [403, 302, 401, 422]:
                    logger.warning(f"–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ {response.status_code}")
                    self.failed_requests_count += 1
                    time.sleep(random.uniform(2.0, 5.0))
                    if time.time() - self.last_ip_change > 15:
                        if self.change_ip(max_attempts=2):
                            self.last_ip_change = time.time()
                            continue
                    raise RequestsError(f"–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {response.status_code}")
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å {response.status_code}")
                self.failed_requests_count += 1
                raise RequestsError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            except (RequestsError, Exception) as e:
                error_msg = str(e)
                mult = 3 if any(k in error_msg.upper() for k in ["SSL", "TIMEOUT", "CONNECTION"]) else 1
                sleep_time = backoff_factor * attempt * mult + random.uniform(0.1, 0.6)
                if self.failed_requests_count >= 2 and time.time() - self.last_ip_change > 15:
                    if self.change_ip(max_attempts=2):
                        self.last_ip_change = time.time()
                        continue
                if attempt < retries:
                    time.sleep(min(sleep_time, 15))
                else:
                    return None

    def parse(self):
        self.load_cookies()
        profiles = self.db.conn.execute("SELECT id, client_id, client_secret, token FROM profiles").fetchall()
        throttle_factor = 1.0
        for profile in profiles:
            profile_id, client_id, client_secret, token = profile
            ads = self.db.conn.execute("SELECT id, category, max_price, target_place_start, target_place_end, comment, url FROM ads WHERE profile_id = ? AND active = TRUE", (profile_id,)).fetchall()
            ads = list(ads)
            random.shuffle(ads)
            for ad in ads:
                ad_id, category, max_price, target_place_start, target_place_end, comment, url = ad
                last_stat = self.db.conn.execute(
                    "SELECT price FROM ad_stats WHERE ad_id = ? ORDER BY timestamp DESC LIMIT 1",
                    (ad_id,)
                ).fetchone()
                if last_stat and last_stat[0] is not None:
                    price_of_view = int(last_stat[0])
                else:
                    bid_info = get_bid_info(token, ad_id)
                    if bid_info and bid_info.get('manual', {}).get('minBidPenny') is not None:
                        price_of_view = bid_info.get('manual', {}).get('minBidPenny')
                    else:
                        price_of_view = 0
                target_id = ad_id
                for pages in range(1, 3):
                    if self.stop_event and self.stop_event.is_set():
                        return
                    time.sleep(random.uniform(1.2, 2.8) * throttle_factor)
                    html_code = self.fetch_data(url=category, retries=self.config.max_count_of_retry)
                    if not html_code:
                        throttle_factor = min(throttle_factor * 1.3, 5.0)
                        break
                    throttle_factor = max(throttle_factor * 0.95, 0.7)
                    data_from_page = self.find_json_on_page(html_code=html_code)
                    try:
                        ads_models = ItemsResponse(**data_from_page.get("catalog", {}))
                    except ValidationError as err:
                        logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {err}")
                        break
                    ads_list = self._clean_null_ads(ads=ads_models.items)
                    current_index = self.find_place_of_target_ad(target_id, ads_list)
                    print("id: ", target_id)
                    print("Current index: ", current_index)
                    
                    if current_index != 0:
                        self.db.insert_ad_stat(ad_id, price_of_view, current_index)
                        break
                    if pages == 2:
                        self.db.insert_ad_stat(ad_id, price_of_view, 100)
                        break
                    url = self.get_next_page_url(url=url)

    @staticmethod
    def _clean_null_ads(ads: list[Item]) -> list[Item]:
        return [ad for ad in ads if ad.id]

    @staticmethod
    def find_place_of_target_ad(target_id, ads):
        for i, ad in enumerate(ads):
            if str(ad.id) == str(target_id):
                return i + 1
        return 0

    @staticmethod
    def extarct_ad_id(s):
        return s[:s.index('?')].split('_')[-1] if '?' in s else s.split('_')[-1]

    @staticmethod
    def find_json_on_page(html_code, data_type: str = "mime") -> dict:
        soup = BeautifulSoup(html_code, "html.parser")
        try:
            for _script in soup.select('script'):
                if data_type == 'mime':
                    if _script.get('type') == 'mime/invalid' and _script.get('data-mfe-state') == 'true':
                        mime_data = json.loads(html.unescape(_script.text)).get('data', {})
                        return mime_data
        except Exception as err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {err}")
        return {}

    def change_ip(self, max_attempts: int = 3) -> bool:
        if not self.mobile_proxies:
            logger.warning("‚ö†Ô∏è –°–º–µ–Ω–∞ IP –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞ - –º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            return False
        current_proxy = self.mobile_proxies[self.current_proxy_index]
        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —Å–º–µ–Ω—É IP –¥–ª—è –ø—Ä–æ–∫—Å–∏ {current_proxy.name}...")
        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ —Å–º–µ–Ω—ã IP {attempt}/{max_attempts} –Ω–∞ –ø—Ä–æ–∫—Å–∏ {current_proxy.name}")
                res = requests.get(
                    url=current_proxy.proxy_change_url,
                    timeout=20,
                    verify=False
                )
                if res.status_code == 200:
                    self.requests_count = 0
                    self.failed_requests_count = 0
                    self.proxy_requests_count = 0
                    wait_time = random.randint(1, 5)
                    logger.info(f"‚úÖ IP —É—Å–ø–µ—à–Ω–æ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ –ø—Ä–æ–∫—Å–∏ {current_proxy.name}! –ü–∞—É–∑–∞ {wait_time} —Å–µ–∫ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏")
                    time.sleep(wait_time)
                    logger.info("üç™ –û–±–Ω–æ–≤–ª—è—é cookies —Å –Ω–æ–≤—ã–º IP...")
                    self.cookies = self.get_cookies(max_retries=2)
                    return True
                else:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã IP –Ω–∞ –ø—Ä–æ–∫—Å–∏ {current_proxy.name}: —Å—Ç–∞—Ç—É—Å {res.status_code}")
            except Exception as err:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ IP –Ω–∞ –ø—Ä–æ–∫—Å–∏ {current_proxy.name} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {err}")
            if attempt < max_attempts:
                wait_time = random.randint(1, 2) * attempt
                logger.info(f"–ü–æ–≤—Ç–æ—Ä —Å–º–µ–Ω—ã IP —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫—É–Ω–¥...")
                time.sleep(wait_time)
        logger.warning("–°–º–µ–Ω–∞ IP –Ω–∞ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–∫—Å–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø—Ä–æ–±—É–µ–º —Ä–æ—Ç–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏...")
        if len(self.mobile_proxies) > 1 and getattr(self.config, 'proxy_switch_on_error', True):
            if self.rotate_proxy():
                logger.info("üîÑ –ü—Ä–æ–∫—Å–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω, –ø—Ä–æ–±—É–µ–º —Å–º–µ–Ω—É IP –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–æ–∫—Å–∏...")
                new_proxy = self.mobile_proxies[self.current_proxy_index]
                try:
                    res = requests.get(
                        url=new_proxy.proxy_change_url,
                        timeout=20,
                        verify=False
                    )
                    if res.status_code == 200:
                        self.requests_count = 0
                        self.failed_requests_count = 0
                        self.proxy_requests_count = 0
                        wait_time = random.randint(1, 5)
                        logger.info(f"‚úÖ IP —É—Å–ø–µ—à–Ω–æ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–æ–∫—Å–∏ {new_proxy.name}! –ü–∞—É–∑–∞ {wait_time} —Å–µ–∫")
                        time.sleep(wait_time)
                        self.cookies = self.get_cookies(max_retries=2)
                        return True
                except Exception as err:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã IP –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–æ–∫—Å–∏ {new_proxy.name}: {err}")
        logger.error("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —Å–º–µ–Ω—ã IP –∏ —Ä–æ—Ç–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã!")
        logger.info("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ cookies...")
        self.cookies = self.get_cookies(max_retries=1)
        return False

    @staticmethod
    def get_next_page_url(url: str):
        try:
            url_parts = urlparse(url)
            query_params = parse_qs(url_parts.query)
            current_page = int(query_params.get('p', [1])[0])
            query_params['p'] = current_page + 1
            new_query = urlencode(query_params, doseq=True)
            next_url = urlunparse((url_parts.scheme, url_parts.netloc, url_parts.path, url_parts.params, new_query, url_parts.fragment))
            return next_url
        except Exception as err:
            logger.error(f"–ù–µ —Å–º–æ–≥ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è {url}. –û—à–∏–±–∫–∞: {err}")

import requests  # —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è curl_cffi.requests –≤—ã—à–µ; —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏—à–Ω–∏–º –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ requests, –æ—Å—Ç–∞–≤–ª–µ–Ω –µ—Å–ª–∏ –≥–¥–µ-—Ç–æ –Ω—É–∂–µ–Ω



if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–¥")
    init_db_from_config()
    print("–£—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    while True:
        try:
            config = load_avito_config("config.json")
            parser = AvitoParse(config)
            parser.parse()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–∞—É–∑—É (–∑–∞—â–∏—Ç–∞ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö/None –∑–Ω–∞—á–µ–Ω–∏–π, –≤—ã–∑—ã–≤–∞—é—â–∏—Ö OSError: [Errno 22] Invalid argument)
            raw_pause = getattr(config, 'pause_general', 60)
            try:
                pause_val = int(raw_pause)
            except Exception:
                pause_val = 60
            if pause_val < 0:
                logger.warning(f"–ü–æ–ª—É—á–µ–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ pause_general={raw_pause}. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 30 —Å–µ–∫")
                pause_val = 30

            logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–∞—É–∑–∞ {pause_val} —Å–µ–∫")
            print("Updating prices")
            check_and_update_prices()
            
            try:
                time.sleep(pause_val)
            except OSError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–Ω–∞ (pause={pause_val}): {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø–∞—Å–Ω—É—é –ø–∞—É–∑—É 30 —Å–µ–∫")
                time.sleep(30)
            
        except Exception as err:
            # –ü–æ–ª–Ω—ã–π traceback –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            logger.exception(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {err}")
            error_msg = str(err)
            try:
                if config and hasattr(config, 'proxy_change_url') and config.proxy_change_url:
                    logger.warning("–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —Å–º–µ–Ω–∞ IP –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                    emergency_parser = AvitoParse(config)
                    emergency_parser.change_ip(max_attempts=3)
            except:
                pass
            
            # –î–ª—è SSL –æ—à–∏–±–æ–∫ –¥–µ–ª–∞–µ–º –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω—É—é –ø–∞—É–∑—É
            if any(keyword in error_msg for keyword in ["SSL", "UNEXPECTED_EOF", "Max retries exceeded", "Connection"]):
                logger.warning("üåê –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –¥–æ 90 —Å–µ–∫—É–Ω–¥")
                time.sleep(60)
            else:
                time.sleep(30)
